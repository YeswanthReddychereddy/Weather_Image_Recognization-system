{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fabd3aba-2d82-4827-a1fb-a8d3e2693bee",
   "metadata": {},
   "source": [
    "### Model Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b818b654-b3fa-4c6b-aaba-7564345c6e4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Set the path to your Kaggle dataset\n",
    "train_data_dir = '///Users/chsmac/Downloads/dataset'\n",
    "\n",
    "# Define parameters\n",
    "img_height, img_width = 128, 128\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "# Use data augmentation to enhance the training dataset\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2  # Split 20% of the data for validation\n",
    ")\n",
    "\n",
    "# Load the training dataset\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'  # Use the training subset\n",
    ")\n",
    "\n",
    "# Load the validation dataset\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'  # Use the validation subset\n",
    ")\n",
    "\n",
    "# Define the CNN model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(len(train_generator.class_indices), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save('weather_image_recognition_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43ae172-1dc6-4e47-bf49-51818abac4ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534501d7-16a0-4e4b-b07e-7625206310db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "evaluation_results = model.evaluate(validation_generator)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Evaluation Result - Loss: {:.4f}, Accuracy: {:.4f}\".format(evaluation_results[0], evaluation_results[1]))\n",
    "\n",
    "\n",
    "# Plot the training and validation accuracy curves\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy Curves')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccacf4d3-df03-44f0-b9db-49a66cfb4288",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5627b5-e32f-434e-a9e1-dad311bac463",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = pymongo.MongoClient(\"mongodb://localhost:27017\")\n",
    "\n",
    "# Create or select a database\n",
    "db = client[\"weather_recognition_db\"]\n",
    "\n",
    "# Create or select a collection\n",
    "collection = db[\"weather_images\"]\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('weather_image_recognition_model.h5')\n",
    "\n",
    "# Define a function to predict weather from an image\n",
    "def predict_weather(image_path):\n",
    "    img = image.load_img(image_path, target_size=(128, 128))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0) / 255.0  # Normalize the image\n",
    "    prediction = model.predict(img_array)\n",
    "    predicted_class = np.argmax(prediction)\n",
    "    return predicted_class\n",
    "\n",
    "# Specify the directory containing weather images\n",
    "images_directory = '///Users/chsmac/Downloads/dataset'\n",
    "\n",
    "# Iterate over images in the directory and store predictions in MongoDB\n",
    "for image_filename in os.listdir(images_directory):\n",
    "    if image_filename.endswith(\".jpg\"):\n",
    "        image_path = os.path.join(images_directory, image_filename)\n",
    "        predicted_weather = predict_weather(image_path)\n",
    "\n",
    "        # Insert data into MongoDB\n",
    "        image_data = {\n",
    "            \"filename\": image_filename,\n",
    "            \"path\": image_path,\n",
    "            \"predicted_weather_class\": int(predicted_weather),\n",
    "            \"predicted_weather_label\": \"sunny\" if predicted_weather == 0 else \"cloudy\" if predicted_weather == 1 else \"rainy\"\n",
    "        }\n",
    "        collection.insert_one(image_data)\n",
    "\n",
    "# Close the MongoDB connection\n",
    "client.close()\n",
    "\n",
    "#print('jdj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c146a48-e7f8-4cad-bcc1-2e726e8a598b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install pymongo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52939d00-5686-46b4-a4ea-0a63729cce25",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb53b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4deefbd",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.1) /Users/xperience/GHA-OpenCV-Python/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/resize.cpp:4062: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m#images\u001b[39;00m\n\u001b[1;32m     41\u001b[0m base_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/chsmac/Desktop/DATA 603 Project/dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 43\u001b[0m image_df \u001b[38;5;241m=\u001b[39m load_and_process_images(base_directory)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m#training and test sets\u001b[39;00m\n\u001b[1;32m     46\u001b[0m train_df, test_df \u001b[38;5;241m=\u001b[39m image_df\u001b[38;5;241m.\u001b[39mrandomSplit([\u001b[38;5;241m0.6\u001b[39m, \u001b[38;5;241m0.4\u001b[39m])\n",
      "Cell \u001b[0;32mIn[3], line 34\u001b[0m, in \u001b[0;36mload_and_process_images\u001b[0;34m(base_directory)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjpg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjpeg\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[1;32m     33\u001b[0m             file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dir_path, file)\n\u001b[0;32m---> 34\u001b[0m             processed_img \u001b[38;5;241m=\u001b[39m process_image(file_path)\n\u001b[1;32m     35\u001b[0m             image_data\u001b[38;5;241m.\u001b[39mappend((label, Vectors\u001b[38;5;241m.\u001b[39mdense(processed_img)))\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(image_data, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[3], line 18\u001b[0m, in \u001b[0;36mprocess_image\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_image\u001b[39m(file_path):\n\u001b[1;32m     17\u001b[0m     img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(file_path, cv2\u001b[38;5;241m.\u001b[39mIMREAD_GRAYSCALE)  \u001b[38;5;66;03m# Convert to grayscale\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(img, (\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m))  \u001b[38;5;66;03m# Resize\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mflatten() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m  \u001b[38;5;66;03m# Normalize\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.8.1) /Users/xperience/GHA-OpenCV-Python/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/resize.cpp:4062: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import FloatType, ArrayType, StringType\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Spark Session\n",
    "spark = SparkSession.builder.appName(\"ImageClassification\").getOrCreate()\n",
    "\n",
    "#preprocess images\n",
    "def process_image(file_path):\n",
    "    img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)  # Convert to grayscale\n",
    "    img = cv2.resize(img, (32, 32))  # Resize\n",
    "    img = img.flatten() / 255.0  # Normalize\n",
    "    return img.tolist()\n",
    "\n",
    "#load and process images from subdirectories\n",
    "def load_and_process_images(base_directory):\n",
    "    categories = [\"dew\", \"fogsmog\", \"frost\",\"glaze\",\"hail\",\"lightning\",\"rain\",\"rainbow\",\"rime\",\"sandstorm\",\"snow\"]  # Subfolder names\n",
    "    image_data = []\n",
    "\n",
    "    for category in categories:\n",
    "        dir_path = os.path.join(base_directory, category)\n",
    "        label = categories.index(category)  # Assign a numerical label based on category\n",
    "\n",
    "        for file in os.listdir(dir_path):\n",
    "            if file.endswith(('jpg', 'png', 'jpeg')):\n",
    "                file_path = os.path.join(dir_path, file)\n",
    "                processed_img = process_image(file_path)\n",
    "                image_data.append((label, Vectors.dense(processed_img)))\n",
    "\n",
    "    \n",
    "    return spark.createDataFrame(image_data, [\"label\", \"features\"])\n",
    "\n",
    "#images\n",
    "base_directory = \"/Users/chsmac/Desktop/DATA 603 Project/dataset\"\n",
    "\n",
    "image_df = load_and_process_images(base_directory)\n",
    "\n",
    "#training and test sets\n",
    "train_df, test_df = image_df.randomSplit([0.6, 0.4])\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\n",
    "pipeline = Pipeline(stages=[rf])\n",
    "\n",
    "model = pipeline.fit(train_df)\n",
    "\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "# Evaluate\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Test Accuracy = {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfbe3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "predictions_with_accuracy = predictions.withColumn('is_correct', when(predictions.label == predictions.prediction, 1).otherwise(0))\n",
    "\n",
    "\n",
    "correct_predictions = predictions_with_accuracy.filter(predictions_with_accuracy.is_correct == 1).count()\n",
    "incorrect_predictions = predictions_with_accuracy.filter(predictions_with_accuracy.is_correct == 0).count()\n",
    "\n",
    "print(f\"Correctly identified images: {correct_predictions}\")\n",
    "print(f\"Incorrectly identified images: {incorrect_predictions}\")\n",
    "\n",
    "\n",
    "pandas_df = predictions_with_accuracy.toPandas()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(['Correct', 'Incorrect'], [correct_predictions, incorrect_predictions], color=['green', 'red'])\n",
    "plt.title('Classification Results')\n",
    "plt.xlabel('Type')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "#confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "cm = confusion_matrix(pandas_df['label'], pandas_df['prediction'])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues, ax=plt.subplot(1, 2, 2))\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53eba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_base_directory = \"/Users/kaushikmanjunatha/Downloads/data_animal/Test\"\n",
    "\n",
    "\n",
    "def extract_labels_from_filenames(filename):\n",
    "    # Assuming your file names have labels separated by underscores (e.g., \"cat_001.jpg\")\n",
    "    label = filename.split(\"_\")[0]\n",
    "    return label\n",
    "\n",
    "def load_and_process_test_images(base_directory):\n",
    "    image_data = []\n",
    "\n",
    "    for file in os.listdir(base_directory):\n",
    "        if file.endswith(('jpg', 'png', 'jpeg')):\n",
    "            file_path = os.path.join(base_directory, file)\n",
    "            processed_img = process_image(file_path)\n",
    "            label = extract_labels_from_filenames(file)  # Extract label from file name\n",
    "            image_data.append((label, Vectors.dense(processed_img)))\n",
    "\n",
    "\n",
    "    return spark.createDataFrame(image_data, [\"true_category\", \"features\"])\n",
    "\n",
    "\n",
    "test_df = load_and_process_test_images(test_base_directory)\n",
    "\n",
    "test_predictions = model.transform(test_df)\n",
    "\n",
    "\n",
    "test_predictions.select(\"true_category\", \"prediction\").show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
